name: Fraud Detection CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  PYTHON_VERSION: "3.9"
  MODEL_REGISTRY: "fraud-detection-models"

jobs:
  # ===== DATA VALIDATION =====
  data-validation:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Validate data schema
      run: |
        python -c "
        import pandas as pd
        import numpy as np
        from src.preprocessing import FeaturePreprocessor
        
        print('Data validation ba≈ülatƒ±lƒ±yor...')
        
        # Synthetic data ile test
        np.random.seed(42)
        data = {
            'Amount': np.random.lognormal(2, 1, 100),
            'Time': np.random.randint(0, 86400, 100),
            'Class': np.random.choice([0, 1], 100, p=[0.95, 0.05])
        }
        df = pd.DataFrame(data)
        
        # Schema validation
        required_columns = ['Amount', 'Time', 'Class']
        assert all(col in df.columns for col in required_columns), 'Missing required columns'
        
        # Data type validation
        assert df['Amount'].dtype in ['int64', 'float64'], 'Amount must be numeric'
        assert df['Class'].dtype in ['int64', 'float64'], 'Class must be numeric'
        
        # Value range validation
        assert df['Amount'].min() >= 0, 'Amount cannot be negative'
        assert df['Class'].isin([0, 1]).all(), 'Class must be 0 or 1'
        
        print('‚úÖ Data validation passed')
        "

  # ===== CODE QUALITY =====
  code-quality:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        # Load and verify models
        models = ['random_forest', 'logistic_regression']
        model_info = {}
        
        for model_name in models:
            model = joblib.load(f'models/{model_name}_model.pkl')
            model_info[model_name] = {
                'type': type(model).__name__,
                'status': 'deployed'
            }
            print(f'‚úÖ {model_name} deployed to production')
        
        # Create deployment manifest
        manifest = {
            'deployment_id': f'deploy_{int(time.time())}',
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'models': model_info,
            'version': '1.0.0'
        }
        
        with open('deployment_manifest.json', 'w') as f:
            json.dump(manifest, f, indent=2)
        
        print('‚úÖ Production deployment completed')
        print(f'Deployment manifest: {json.dumps(manifest, indent=2)}')
        "
    
    - name: Upload deployment manifest
      uses: actions/upload-artifact@v3
      with:
        name: deployment-manifest
        path: deployment_manifest.json

  # ===== POST-DEPLOYMENT TESTS =====
  post-deployment-tests:
    runs-on: ubuntu-latest
    needs: [deploy-production]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Download model artifacts
      uses: actions/download-artifact@v3
      with:
        name: trained-models
        path: models/
    
    - name: Run post-deployment tests
      run: |
        python -c "
        import joblib
        import numpy as np
        import pandas as pd
        import time
        
        print('Running post-deployment tests...')
        
        # Load models
        rf_model = joblib.load('models/random_forest_model.pkl')
        lr_model = joblib.load('models/logistic_regression_model.pkl')
        preprocessor = joblib.load('models/preprocessor.pkl')
        
        # Test data
        test_data = {
            'Amount': [100.0, 5000.0, 50.0],
            'Time': [14400, 72000, 3600],
            'Merchant_Category': ['grocery', 'online', 'gas']
        }
        df_test = pd.DataFrame(test_data)
        
        # Preprocess
        X_test = preprocessor.transform(df_test)
        
        # Test predictions
        rf_pred = rf_model.predict_proba(X_test)
        lr_pred = lr_model.predict_proba(X_test)
        
        print('‚úÖ Random Forest predictions:', rf_pred[:, 1])
        print('‚úÖ Logistic Regression predictions:', lr_pred[:, 1])
        
        # Performance test
        start_time = time.time()
        for _ in range(100):
            rf_model.predict_proba(X_test)
        inference_time = (time.time() - start_time) / 100
        
        print(f'‚úÖ Average inference time: {inference_time*1000:.2f}ms')
        
        # Assert performance requirements
        assert inference_time < 0.1, f'Inference too slow: {inference_time:.4f}s'
        
        print('‚úÖ All post-deployment tests passed')
        "

  # ===== MONITORING SETUP =====
  setup-monitoring:
    runs-on: ubuntu-latest
    needs: [post-deployment-tests]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    steps:
    - name: Setup monitoring
      run: |
        echo "üìä Setting up production monitoring..."
        
        python -c "
        import json
        import time
        
        # Monitoring configuration
        monitoring_config = {
            'alerts': {
                'model_drift': {'threshold': 0.1, 'enabled': True},
                'performance_degradation': {'threshold': 0.05, 'enabled': True},
                'high_fraud_rate': {'threshold': 0.2, 'enabled': True}
            },
            'metrics': {
                'prediction_latency': {'target': '<100ms'},
                'throughput': {'target': '>1000/min'},
                'accuracy': {'target': '>95%'}
            },
            'dashboards': ['model_performance', 'fraud_detection_rates', 'system_health']
        }
        
        print('Monitoring configuration:')
        print(json.dumps(monitoring_config, indent=2))
        print('‚úÖ Monitoring setup completed')
        "

# ===== NOTIFICATION =====
  notify:
    runs-on: ubuntu-latest
    needs: [setup-monitoring]
    if: always()
    steps:
    - name: Notify deployment status
      run: |
        if [ "${{ needs.setup-monitoring.result }}" == "success" ]; then
          echo "‚úÖ Fraud Detection Pipeline: SUCCESS"
          echo "üöÄ Production deployment completed successfully"
          echo "üìä Monitoring is active"
        else
          echo "‚ùå Fraud Detection Pipeline: FAILED"
          echo "‚ö†Ô∏è  Check pipeline logs for details"
        fi